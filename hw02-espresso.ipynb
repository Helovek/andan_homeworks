{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9074d943",
      "metadata": {
        "id": "9074d943"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/FUlyankin/r_probability/master/end_seminars_2020/sem08/real_expect.png\" width=\"800\">\n",
        "\n",
        "# Андан на экономе: домашнее задание 2\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "153348a7",
      "metadata": {
        "id": "153348a7"
      },
      "source": [
        "**ФИО:** Поздняков\n",
        "\n",
        "**[2 бонусных балла] прикрепите к работе самый крутой мем из своих сохранёнок:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f40dc119",
      "metadata": {
        "id": "f40dc119"
      },
      "source": [
        "## Общая информация\n",
        "\n",
        "__Дата выдачи:__ 27.01.2025\n",
        "\n",
        "__Мягкий дедлайн:__ 23:59MSK 23.02.2025\n",
        "\n",
        "__Жесткий дедлайн:__ 23:59MSK 02.03.2025"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5147913f",
      "metadata": {
        "id": "5147913f"
      },
      "source": [
        "В этой домашке мы с вами:\n",
        "\n",
        "1. Посмотрим какимы бывают доверительные интервалы для долей\n",
        "2. Вспомним квантильное преобразование и сгенерируем распределение максимума\n",
        "3. Построим метрику плохих показов, происследуем её на смещенность и исправим её"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad3cbdbe",
      "metadata": {
        "id": "ad3cbdbe"
      },
      "source": [
        "## Задача 1: Доверительные интервалы для долей (30 баллов)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56665b52",
      "metadata": {
        "id": "56665b52"
      },
      "source": [
        "В этой задаче вам предстоит поработать с разными видами доверительных интервалов для долей.\n",
        "\n",
        "Пусть случайные величины $X_1, \\ldots, X_n$ независимы и имеют распределение Бернулли с неизвестным параметром $p$, при этом $\\hat p = \\bar x$.\n",
        "\n",
        "__а) [10 баллов]__ Если воспользоваться ЦПТ, можно построить для неизвестного $p$ доверительный интервал Вальда. На паре мы для этого использовали сходимость\n",
        "\n",
        "$$\n",
        "\\frac{\\hat p - p}{\\sqrt{\\frac{\\hat p (1 - \\hat p)}{n}}} \\to N(0;1),\n",
        "$$\n",
        "\n",
        "а затем решали неравенство\n",
        "$$\n",
        "-z_{cr} \\leq \\frac{\\hat p - p}{\\sqrt{\\frac{\\hat p (1 - \\hat p)}{n}}} \\leq z_{cr}.\n",
        "$$\n",
        "\n",
        "Получалось, что при уровне значимости $\\alpha,$ доверительный интервал\n",
        "\n",
        "$$\n",
        "\\hat p \\pm z_{1 - \\frac{\\alpha}{2}} \\cdot \\sqrt{ \\frac{\\hat p \\cdot (1 - \\hat p)}{n} }\n",
        "$$\n",
        "\n",
        "накрывает неизвестное значение $p$ с вероятностью $1 - \\alpha$.\n",
        "\n",
        "В этом задании мы, с помощью симуляций на компьютере, сравним фактическую вероятность накрытия неизвестного параметра $p$ интервалами Вальда, Вильсона и Агрести—Коулла c номинальной 95\\%-й вероятностью."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e52b0e8",
      "metadata": {
        "id": "7e52b0e8"
      },
      "source": [
        "### Сеттинг симуляций:\n",
        "\n",
        "1. Зафиксируем уровень значимости равный $5\\%$\n",
        "2. Будем перебирать $p$ в интервале $(0, 1)$ с шагом в 0.01\n",
        "3. Для каждого p проведем $1000$ симуляций\n",
        "    - генерируем с заданным $p$ выборку размера $n = 10$\n",
        "    - строим доверительный интервал для $p$\n",
        "    - cмотрим попадает ли истинное значение в доверительный интервал\n",
        "4. Оцениваем по симуляциям получившийся уровень значимости (доля случаев, когда доверительный интервал не покрыл $p$).\n",
        "5. Рисуем картинку, где по оси абсцис отложены значения $p$, а по оси ординат полученная оценка уровня значимости. Отдельной пунктирной линией рисуем на картинке $0.05$.\n",
        "\n",
        "Картинка должна получиться примерно такой же как [в этой статье.](https://www.ime.usp.br/~jmsinger/Textos/Castroetal2019.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4fd7177",
      "metadata": {
        "id": "f4fd7177"
      },
      "outputs": [],
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
        "# will the code be with you"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eccf78bc",
      "metadata": {
        "id": "eccf78bc"
      },
      "source": [
        "__б) [5 баллов]__ Можно не заменять в дисперсии значения $p$ на $\\hat p$, а вместо этого воспользоваться сходимостью\n",
        "\n",
        "$$\n",
        "\\frac{\\hat p - p}{\\sqrt{\\frac{p (1 - p)}{n}}} \\to N(0;1)\n",
        "$$\n",
        "\n",
        "и решить относительно неизвестного $p$ (о ужас!) квадратное неравенство. Выйдет доверительный интервал Уилсона:\n",
        "\n",
        "$$\n",
        "\\hat p_w \\pm z_{1 - \\frac{\\alpha}{2}} \\cdot \\sqrt{\\frac{u \\hat p (1 - \\hat p) + (1 - u) (1/2)^2 }{n_w}},\n",
        "$$\n",
        "где $\\quad$ $\\hat p_w = u \\hat p + (1 - u) (1/2)$, $\\quad$  $u = \\frac{n}{n + z_{1 - \\frac{\\alpha}{2}}^2}$, $\\quad$ $(1-u) = \\frac{z_{1 - \\frac{\\alpha}{2}}^2}{n + z_{1 - \\frac{\\alpha}{2}}^2}$,  $\\quad$\n",
        "  $n_w = n + z_{1 - \\frac{\\alpha}{2}}^2$.\n",
        "\n",
        "**Необязательно:** с помощью ручки и бумаги убедитесь, что эти формулы верны."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c93c2208",
      "metadata": {
        "id": "c93c2208"
      },
      "outputs": [],
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
        "# will the code be with you"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f07993a",
      "metadata": {
        "id": "8f07993a"
      },
      "source": [
        "Повторите для доверительного интервала Уилсона тот же самый эксперимент и постройте картинку с поведением уровня значимости в зависимости от значений $p$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad9278f2",
      "metadata": {
        "id": "ad9278f2"
      },
      "outputs": [],
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
        "# will the code be with you"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69c95825",
      "metadata": {
        "id": "69c95825"
      },
      "source": [
        "### Блок необязательных заданий\n",
        "\n",
        "> Эти задания вы можете попытаться решить на бумаге сами. Если вам лень это делать, достаточно их прочитать и как следует осмыслить ответы.\n",
        "\n",
        "**Задание 1:**\n",
        "\n",
        "Обозначим центр интервала Вильсона с помощью $\\hat p_w$. С помощью ручки и  бумаги докажите, что центр интервала Вильсона $\\hat p_w$ можно представить как средневзвешенное классической оценки $\\hat p$ и тривиальной оценки $1/2$,\n",
        "\n",
        "$$\n",
        "\\hat p_w = u \\cdot \\hat p + (1 - u) \\cdot (1/2).  \n",
        "$$\n",
        "\n",
        "Найдите веса $u$ и $(1-u)$.\n",
        "\n",
        "**Ответ:**  $u = \\frac{n}{n + z_{cr}^2}$, $\\quad$ $(1-u) = \\frac{z_{cr}^2}{n + z_{cr}^2}$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da0b7c72",
      "metadata": {
        "id": "da0b7c72"
      },
      "source": [
        "**Задание 2:**  Докажите, что центр интервала Вильсона $\\hat p_w$ можно проинтерпретировать следующим образом: добавим $f$ вымышленных $1$ и $f$ вымышленных $0$ в выборку и посчитаем классическую оценку вероятности для выборки с вымышленными наблюдениями,\n",
        "\n",
        "$$\n",
        "\\hat p_w = \\frac{\\sum_{i=1}^n Y_i + f}{n + 2 f}.\n",
        "$$\n",
        "\n",
        "Какому целому числу примерно равно $f$ для 95\\%-го доверительного интервала?\n",
        "\n",
        "__Ответ:__\n",
        "\n",
        "$$\n",
        "\\hat p_w = \\frac{\\sum Y_i + z_{cr}^2/2}{n + z_{cr}^2},\n",
        "$$\n",
        "\n",
        "то есть мы добавляем в выборку $f = z_{cr}^2/2$ вымышленных $1$ и столько же вымышленных $0$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40641826",
      "metadata": {
        "id": "40641826"
      },
      "source": [
        "**Задание 3:**  Докажите, что интервал Вильсона можно записать в виде\n",
        "\n",
        "$$\n",
        "\\hat p_w \\pm z_{cr} \\cdot \\sqrt{\\frac{u \\cdot \\hat p (1 - \\hat p) + (1 - u) \\cdot (1/2)^2 }{n_w}}.\n",
        "$$\n",
        "\n",
        "Найдите $n_w$, а также веса $u$ и $(1 - u)$.\n",
        "\n",
        "__Ответ:__\n",
        "\n",
        "$\\hat p_w = u \\cdot \\hat p + (1 - u) \\cdot (1/2)$, $\\quad$ $u = \\frac{n}{n + z_{cr}^2}$, $\\quad$  $(1-u) = \\frac{z_{cr}^2}{n + z_{cr}^2}$, $\\quad$ $n_w = n + z_{cr}^2$.\n",
        "\n",
        "Таким образом, интервал Вильсона слегка корректирует число наблюдений и использует в качестве оценки дисперсии $X_i$ средневзвешенное между классической оценкой $\\hat p (1 - \\hat p)$ и тривиальной оценкой $1/4$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4737298",
      "metadata": {
        "id": "a4737298"
      },
      "source": [
        "__в) [5 баллов]__ Доверительный интервал Агрести—Коулла для уровня доверия 95\\% строится следующим образом.\n",
        "В выборку мысленно добавляют два наблюдения равных $1$ и два наблюдений равных $0$,\n",
        "считают оценку доли\n",
        "$$\n",
        "\\hat p_{ac} = \\frac{\\sum_{i=1}^n Y_i + 2}{n + 4},  \n",
        "$$\n",
        "а затем строят классический интервал Вальда, используя $\\hat p_{ac}$ вместо классической $\\hat p$.\n",
        "\n",
        "Повторите для доверительного интервала Агрести—Коулла тот же самый эксперимент и постройте картинку с поведением уровня значимости в зависимости от значений $p$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa20c467",
      "metadata": {
        "id": "fa20c467"
      },
      "outputs": [],
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
        "# will the code be with you"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a918892",
      "metadata": {
        "id": "6a918892"
      },
      "source": [
        "__г) [5 баллов]__ С помощью [метода стабилизации дисперсии](https://youtu.be/JGNwZKwE2h0) для $p$ можно получить следующий доверительный интервал:\n",
        "\n",
        "$$\n",
        "\\sin^2 \\left(\\arcsin \\sqrt{\\hat p} \\pm \\frac{z_{cr}}{2 \\sqrt{n}} \\right).\n",
        "$$\n",
        "\n",
        "Проведите для него тот же самый эксперимент."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "374e07bb",
      "metadata": {
        "id": "374e07bb"
      },
      "outputs": [],
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
        "# will the code be with you"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5345ae3",
      "metadata": {
        "id": "f5345ae3"
      },
      "source": [
        "__д) [5 баллов]__ Дайте развёрнутый ответ на следующие вопросы.\n",
        "\n",
        "1. Правда ли, что при уровне доверия 95\\% центры интервала Агрести — Коулла и Вильсона совпадают?\n",
        "\n",
        "__Ответ:__\n",
        "\n",
        "2. Какой 95\\%-й интервал шире, Агрести—Коулла или Вильсона?\n",
        "\n",
        "__Ответ:__\n",
        "\n",
        "3. Какой 95\\%-й интервал шире, Вальда или основанный на стабилизации дисперсии?\n",
        "\n",
        "__Ответ:__\n",
        "\n",
        "4. Как доверительные интервалы ведут себя на краях? Как они ведут себя в середине?\n",
        "\n",
        "__Ответ:__\n",
        "\n",
        "5. Перезапустите код при $n=100$. Насколько драматично изменилась картинка?\n",
        "\n",
        "__Ответ:__"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65d1f498",
      "metadata": {
        "id": "65d1f498"
      },
      "source": [
        "Что почитать:\n",
        "\n",
        "- [Статья Денга про дельта-метод](https://alexdeng.github.io/public/files/kdd2018-dm.pdf)\n",
        "- [Выводим распределение для дисперсии и строим асимптотические доверительные интервалы](https://www.stat.umn.edu/geyer/s06/5102/notes/ci.pdf) (тут же есть про стабилизацию дисперсии и многое другое)\n",
        "- [Доверительный интервал Уилсона](https://www.econometrics.blog/post/the-wilson-confidence-interval-for-a-proportion/)\n",
        "- [Доверительный интервал Агрести-Коула](https://www.econometrics.blog/post/don-t-use-the-textbook-ci-for-a-proportion/)\n",
        "- [Сравниваем разные доверительные интервалы для долей](https://www.ime.usp.br/~jmsinger/Textos/Castroetal2019.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "079ba3af",
      "metadata": {
        "id": "079ba3af"
      },
      "source": [
        "## Задача 2: квантильное преобразование (10 баллов)\n",
        "\n",
        "Компьютер хорошо умеет генерировать равномерные случайные величины. Для разных популярных распределений, вроде нормального, люди также смогли придумать хорошие генераторы. Но что делать, если нам хочется научиться генерировать что-то специфическое? В этом людям помогает квантильное преобразование!\n",
        "\n",
        "#### Теорема:\n",
        "\n",
        "Пусть функция распределения $F_X(x)$ непрерывна. Тогда случайная величина $Y = F(X)$ имеет равномерное распределение на отрезке $[0; 1]$.\n",
        "\n",
        "\n",
        "#### Следствие:\n",
        "\n",
        "Пусть $Y \\sim U[0;1]$, а $F(x)$ произвольная функция распределения. Тогда случайная величина $X = F^{-1}(Y)$ будет иметь функцию распределения $F(x)$.\n",
        "\n",
        "\n",
        "#### Что это нам даёт:\n",
        "\n",
        "\n",
        "- Позволяет генерировать из равномерного распределения другие. Достаточно найти обратную функцию и вычислить её значение от каждого элемента в выборке.\n",
        "- Применимо невсегда, напрмер, для нормального распределения используют другие алгоритмы, так как функцию распределения для него невозможно записать аналитически.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/FUlyankin/matstat-AB/main/week05_LLN_CLT/image/quant.png\" height=\"200\">\n",
        "</center>\n",
        "\n",
        "\n",
        "__Полезные видео и код:__\n",
        "\n",
        "- [Видео с объяснением квантильного преобразования](https://www.youtube.com/watch?v=Dxtj-3N22_A&list=PLCf-cQCe1FRyg1ajZ2HJVKknbuTujBOLN&index=9)\n",
        "- [Видео с его использованием в питоне](https://www.youtube.com/watch?v=ivpWyorfWlA&list=PLCf-cQCe1FRyg1ajZ2HJVKknbuTujBOLN&index=10)\n",
        "- [Тетрадка с кодом из видео](https://github.com/FUlyankin/matstat-AB/blob/main/week05_LLN_CLT/09_python_distributions.ipynb)\n",
        "- [Лекция Фила из маги по терверу с подробным разбором квантильного преобразования и выведением функции распределения максимума](https://www.youtube.com/watch?v=vvpRREpe0Bw)\n",
        "- [Выведение функции распределения для максимума в учебнике Черновой](https://tvims.nsu.ru/chernova/tv/lec/node68.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98471f3b",
      "metadata": {
        "id": "98471f3b"
      },
      "source": [
        "Пуcть у нас есть стрёмная функция распределения:\n",
        "\n",
        "$$\n",
        "F(x) = \\left( \\frac{\\ln x}{\\ln \\theta} \\right)^{\\alpha},  \\quad x \\in [1; \\theta]\n",
        "$$\n",
        "\n",
        "__а) [5 баллов]__ Сгенерируйте из него выборку с помощью квантильного преобразования. Параметры $\\alpha$ и $\\theta$ возьмите на свой вкус. Для получившейся выборки постройте гистограмму."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3ff5d0b",
      "metadata": {
        "id": "f3ff5d0b"
      },
      "outputs": [],
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
        "# will the code be with you"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f07c9643",
      "metadata": {
        "id": "f07c9643"
      },
      "source": [
        "__б) [5 баллов]__ Пусть случайные величины $X_1, \\ldots, X_n \\sim U[0;1].$ Пусть $Y_n = max(X_1, \\ldots, X_n).$\n",
        "\n",
        "Мы знаем, что $F_{Y_n}(x) = x^n,$ если $x \\in [0; 1].$\n",
        "\n",
        "Напишите функцию, которая сгенерирует с помощью квантильного преобразования выборку из распределения $F_{Y_n}(t)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0800fbb0",
      "metadata": {
        "id": "0800fbb0"
      },
      "outputs": [],
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
        "# will the code be with you"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6001b408",
      "metadata": {
        "id": "6001b408"
      },
      "source": [
        "## Задача 3: метрика плохих показов (60 баллов)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b865a8e8",
      "metadata": {
        "id": "b865a8e8"
      },
      "source": [
        "Мало того, что в интернете постоянно кто-то не прав, так ещё и куча спама, фейков, хейтспича, кликбейта и другого «плохого» контента. Каждая уважающая себя платформа борется с ним. Более того, гиганты вроде Facebook публикуют [transparency-отчёты](https://transparency.fb.com/reports/community-standards-enforcement/) о том, сколько показов «плохого» контента пропустила их система модерации.\n",
        "\n",
        "Давайте представим себе, что мы Youtube. Мы хотим, чтобы пользователям как можно реже показывался «плохой» контент (например, спам или порно).\n",
        "\n",
        "К сожалению, мы не можем сделать ручную разметку всех видео, которые загружают люди. Поток нового видео на платформе постоянно растёт. Нам надо будет поддерживать огромный штат модераторов. Поэтому для каждого нарушения, с помощью машинного обучения, обычно делают классификаторы. Нейросети пытаются предсказать, есть ли в видео неприемлимый контент. Если его вероятность высокая, видео автоматически банится. Если нейросеть не уверена, видео отправляют на разметку модераторам. Если мы обучим хороший классификатор, подавляющая часть потока будет оставаться без модерации.\n",
        "\n",
        "Параллельно с классификатором мы можем начать размечать модераторами жалобы и самые вирусные видосы. Машинное обучение даёт осечки. Какое-то одно «плохое» видео, пропущенное нашей системой, может набрать много показов. Постмодерация самых вирусных видео и жалоб могут нас от этого спасти.\n",
        "\n",
        "Нам хотелось бы понимать, насколько хорошо работает система модерации. Для этого мы будем оценивать долю «плохих» показов. Если мы показываем видео со спамом, такие показы мы считаем плохими.\n",
        "\n",
        "**Наша задача**  —  получить несмещённую оценку доли плохих показов на youtube, а также построить для неё доверительный интервал. Тогда мы будем видеть, с каким нарушением у нас больше всего проблем, а также сможем придумывать для системы модерации разные улучшения и понимать, насколько они эффективны."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59afac5d",
      "metadata": {
        "id": "59afac5d"
      },
      "source": [
        "#### 1. Данные\n",
        "\n",
        "У нас есть данные о просмотре видео на youtube за сутки. Таблица весьма громадная, так как она содержит несколько миллиардов уникальных видео. Мы хотим выбрать из неё тысячу случайных видео, чтобы модераторы разметили их на спам. При этом, нам нужно учесть частоту просмотров. Популярные видео должны иметь больше шансов попасть в подвыборку. Тогда по этой разметке мы сможем оценить долю плохих показов.\n",
        "\n",
        "|      video                         |            shows                  |\n",
        "|:----------------------------------:|:---------------------------------:|\n",
        "| Baby Shark                         |  13 840 000 000                   |\n",
        "| Despacito                          |   8 340 000 000                   |\n",
        "| Johny Johny Yes Papa               |   6 850 000 000                   |\n",
        "| ........                           |   ........                        |\n",
        "| 1. Андан-2025: симуляции           |   2                               |\n",
        "\n",
        "Давайте посчитаем для каждого видео вероятность, что оно будет показно и сделаем `np.random.choice`, как в коде ниже."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2a6b3bd",
      "metadata": {
        "id": "b2a6b3bd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as sts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e011d0d",
      "metadata": {
        "id": "1e011d0d",
        "outputId": "4e9baf6f-2cee-4045-f52e-a611d66d8260"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([2., 7., 1., ..., 2., 4., 8.])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# сгенерируем показы из распределения Парето, чтобы было побольше выбросов как в жизни.\n",
        "n_obs = 10**6\n",
        "\n",
        "b = 0.7\n",
        "rv = sts.pareto(b)\n",
        "shows = np.round(rv.rvs(n_obs))\n",
        "shows # это показы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc932538",
      "metadata": {
        "id": "cc932538",
        "outputId": "2f580927-11ed-4168-879a-d44538c48a14"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([      1,       2,       3, ...,  999998,  999999, 1000000])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "videos = np.arange(1, n_obs + 1) # это id видео\n",
        "videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89f6421c",
      "metadata": {
        "id": "89f6421c",
        "outputId": "e0cf140f-dbc4-4617-9924-e25f8c45db76"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([3.38036571e-09, 1.18312800e-08, 1.69018286e-09, ...,\n",
              "       3.38036571e-09, 6.76073143e-09, 1.35214629e-08])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "p = shows / shows.sum() # это вероятность того что видео будет показано\n",
        "p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21db5060",
      "metadata": {
        "id": "21db5060",
        "outputId": "09774313-6668-4e54-f803-1c66026b47ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([694444, 741762,  59625, 512650, 903569, 556368, 526257, 105501,\n",
              "       335513, 465649, 238398, 630094, 798148, 704221, 471160, 785462,\n",
              "       210125, 258655, 860784,  45706, 765929, 347988,  73952, 717663,\n",
              "       818720, 315168, 608273, 135060, 217389, 814357, 988670, 836648,\n",
              "       511407, 759351,  31780, 129249,  14373, 345061, 689279, 425168,\n",
              "        93303, 381893, 174987, 143390, 408878, 866738, 359002, 830429,\n",
              "        50139, 163912, 344275, 182960, 325634, 682500, 471509, 122435,\n",
              "       424672,  18357, 676169, 324194, 681674, 214939, 563638, 500804,\n",
              "       201232, 831426, 302568, 163411, 567922, 123939, 502778,  88124,\n",
              "       808625, 193666, 177464, 241919, 496291, 265796, 122676, 541292,\n",
              "       542257, 318294,  58430, 328893, 199800, 865866, 607974, 558085,\n",
              "       889607, 391523, 645997, 134909, 163135, 301019, 194188, 324004,\n",
              "         5286, 706280, 628800, 186296])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# генерируем выборку без повторений из 100 видео с учётом показов (более частые видео попадут в выборку вероятнее)\n",
        "k = 100\n",
        "np.random.choice(videos, size=k, replace=False, p = p)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "021deaf1",
      "metadata": {
        "id": "021deaf1"
      },
      "source": [
        "У такого подхода есть проблема. Если в таблице миллиарды строк, мы не сможем сохранить таблицу в оперативную память. Нам для генерации выборки понадобится супер-компьютер. Хотелось бы этого избежать. К счастью, для решения этой проблемы есть [много алгоритмов,](https://en.wikipedia.org/wiki/Reservoir_sampling) и мы с вами реализуем один из них."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ea21d10",
      "metadata": {
        "id": "6ea21d10"
      },
      "source": [
        "#### 2. Сэмплирование с повторениями\n",
        "\n",
        "Для начала поработаем с сэмплированием с повторениями. Сгенерируем таблицу с данными.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9387a096",
      "metadata": {
        "id": "9387a096",
        "outputId": "73ae7340-a4cb-429a-e8db-91e46bcb0190"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(100000, 3)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_id</th>\n",
              "      <th>shows</th>\n",
              "      <th>is_spam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2129.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>65.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   video_id   shows  is_spam\n",
              "0         1  2129.0        0\n",
              "1         2     2.0        0\n",
              "2         3     1.0        0\n",
              "3         4    65.0        0\n",
              "4         5     7.0        0"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "n_obs = 10**5\n",
        "\n",
        "b = 0.7\n",
        "rv = sts.pareto(b)\n",
        "\n",
        "df = pd.DataFrame.from_dict({\n",
        "    'video_id': np.arange(1, n_obs + 1),\n",
        "    'shows': np.round(rv.rvs(n_obs)),\n",
        "    'is_spam': np.random.binomial(1, 0.30, n_obs), # пусть в 30% видео встречается спам\n",
        "})\n",
        "\n",
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f12344e8",
      "metadata": {
        "id": "f12344e8"
      },
      "source": [
        "__а) [5 баллов]__  Посчитайте по табличке `df` истиную долю плохих показов:\n",
        "\n",
        "$$\n",
        "p_{\\text{bad}} = \\frac{\\sum_{v \\in V} \\text{show}(v) \\cdot \\text{isSpam}(v) }{\\sum_{v \\in V} \\text{show}(v)}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8f9768f",
      "metadata": {
        "id": "f8f9768f"
      },
      "outputs": [],
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
        "# will the code be with you\n",
        "\n",
        "# У вас тут должно выйти что-то похожее на 0.2106, если вы не меняли seed выше"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e574f40",
      "metadata": {
        "id": "8e574f40"
      },
      "source": [
        "Мы не знаем всех меток $\\text{is\\_spam}(v)$. Мы можем позволить себе разметить маленький сэмпл, $S ⊂ V$. Каждое видео попадает к нам в сэмпл пропорционально числу его показов. Поэтому логично оценить долю плохих показов как\n",
        "\n",
        "$$\n",
        "\\hat{p}_{\\text{bad}} = \\frac{1}{|\\text{S}|}\\sum_{v \\in S} \\text{isSpam}(v).\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb2210a2",
      "metadata": {
        "id": "bb2210a2"
      },
      "source": [
        "__б) [10 баллов]__ С помощью функции `np.random.choice` cделайте $10^4$ выборок __с повторениями__ размера $1000$.\n",
        "\n",
        "Постройте для каждой оценку доли плохих показов. Считайте, что модератор во время разметки безошибочно определяет значение из колонки `is_spam`.\n",
        "\n",
        "Нарисуйте гистограмму для получившегося распределения. Отметьте на ней настоящую долю плохих показов и получившееся у вас среднее. Правда ли, что мы получили несмещённую оценку?\n",
        "\n",
        "**Hint:** Перейдите от `pandas` к `numpy` и обратите внимание на команду `argsort()`. Такой код будет работать быстрее."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca9b68e5",
      "metadata": {
        "id": "ca9b68e5"
      },
      "outputs": [],
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
        "# will the code be with you"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4eacebab",
      "metadata": {
        "id": "4eacebab"
      },
      "source": [
        "#### 3. Сэмплирование без повторений\n",
        "\n",
        "Выше мы сказали, что делать `np.random.choice` для огромных таблиц невозможно, так как они не поместятся в оперативную память.\n",
        "\n",
        "Более того, нам надо дать модераторам разметить видео на спам. Многие видео будут повторяться. Уникальных видео каждый раз будет разное количество. Нагрузка на модераторов будет неравномерной. Они будут жаловаться на это. Хочется, чтобы нагрузка всегда была одинаковой.\n",
        "\n",
        "Давайте сменим подход и будем делать **случайную взвешенную выборку БЕЗ ПОВТОРЕНИЙ.** Это означает, что наблюдения зависят друг от друга. Причём корреляция между ними довольно высокая. Это будет приводить к проблемам."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24fcc751",
      "metadata": {
        "id": "24fcc751"
      },
      "source": [
        "Пусть у нас есть $n$ объектов. Мы хотим отобрать $k$ видео для разметки модераторами. Использование `np.random.choice` можно проинтерпретировать следующим образом:\n",
        "\n",
        "1. Напишем название $i-$го видео на разных табличках $\\text{shows}_i$ раз.\n",
        "2. Случайно перемешаем все таблички и положим их в стопку в случайном порядке.\n",
        "3. Будем отбирать самые верхние таблички до тех пор, пока не встретим $k$ разных названий видео."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6d92a13",
      "metadata": {
        "id": "f6d92a13"
      },
      "source": [
        "Если бы нам были бы не важны веса в виде показов, каждое видео попадало бы к нам в выборку равновероятно. Мы могли бы отобрать выборку размера $k$ следущим образом:\n",
        "\n",
        "1. Для каждого видео генерируем $X_i \\sim U[0; 1]$ (по одной табличке на видео).\n",
        "2. Сортируем все видео по сгенерированной случайной величине.\n",
        "3. Срезаем топ-k видео в итоговую выборку."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "461280ff",
      "metadata": {
        "id": "461280ff"
      },
      "source": [
        "Веса в виде показов для нас важны. В теринах равномерных случайных величин алгоритм с весами можно записать так:\n",
        "\n",
        "1. Для $i$-го видео генерируем $\\text{shows}_i$ независимых равномерных случайных величин (для каждого видео свое число табличек).\n",
        "2. Складываем все таблички в одну стопку и сортируем их по сгенерированным величинам.\n",
        "3. Идём по массиву от больших элементов к меньшим и отбираем видео, пока не накопим $k$ элементов."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8e24fc7",
      "metadata": {
        "id": "a8e24fc7"
      },
      "source": [
        "Очень не хочется для какого-нибудь популярного видео генерировать несколько миллиардов случайных чисел. Конечно же, сразу нужно генерировать случайную величину, которая будет максимумом нескольких независимых равномерных случайных величин. На итоговую сортировку это никак не повлияет. Мы отберём те же самые видео.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b90d075",
      "metadata": {
        "id": "4b90d075"
      },
      "source": [
        "Сгенерировать для каждого видео $X_{i,max}$ можно с помощью квантильного преобразования.\n",
        "\n",
        "Пусть случайные величины $X_1, \\ldots, X_m \\sim \\text{iid} \\, U[0;1].$ Пусть $Y = \\max(X_1, \\ldots, X_m).$\n",
        "\n",
        "Мы знаем, что $F_{Y}(x) = x^m,$ если $x \\in [0; 1].$ Выборку из распределения $F_{Y}(x)$ можно сгенерировать в два шага:\n",
        "\n",
        "- $x_1, \\ldots, x_m \\sim \\text{iid} \\, U[0;1]$\n",
        "- $y_i = x_i^{\\frac{1}{m}}$\n",
        "\n",
        "__в) [10 баллов]__ Фактически нам надо отсортировать все видео по величине $X_i^{\\frac{1}{\\text{shows}_i}}$, где $X_i \\sim U[0;1]$, и отобрать топ-$k$ видео в выборку для разметки модераторами. Сделайте, используя эту процедуру, для таблицы `df` сэмпл размера $100$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46cd75c6",
      "metadata": {
        "id": "46cd75c6"
      },
      "outputs": [],
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
        "# will the code be with you"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c38c245f",
      "metadata": {
        "id": "c38c245f"
      },
      "source": [
        "__г) [5 баллов]__ Вычислять на компьютере корни из числа, лежащего между 0 и 1 не очень удобно с точки зрения округления. Могут возникать большие ошибки.\n",
        "\n",
        "Гораздо эффективнее отобрать топ по величине $\\frac{1}{\\text{shows}_i} \\cdot \\ln X_i$, где $X_i \\sim U[0;1]$. Проделайте это."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "199a9358",
      "metadata": {
        "id": "199a9358"
      },
      "outputs": [],
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
        "# will the code be with you"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bd49407",
      "metadata": {
        "id": "7bd49407"
      },
      "source": [
        "Такую процедуру можно найти в продакшн-процессах у многих компаний. В Яндексе похожая процедура используется для сэмплирования поисковых запросов для дальнейшей разметки. [В статье](https://yadi.sk/i/IxKLPFEj3TSpPe) можно найти доказательство того, что алгоритм даст корректную вероятность для каждого видео.\n",
        "\n",
        "В примере выше, мы держим табличку в оперативной памяти компьютера. Это игрушечный пример, и это позволительно. В реальной жизни мы можем считывать строки с жесткого диска, для каждой из них по очереди генерировать случайную величину и поддерживать топ-к уникальных видео в памяти. Например, в этом может помочь такая структура данных [как куча.](https://ru.wikipedia.org/wiki/Куча_(структура_данных))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c725136",
      "metadata": {
        "id": "5c725136"
      },
      "source": [
        "#### 4. Оценка доли плохих показов\n",
        "\n",
        "Дальше нам остаётся разметить видео и аккуратно посчитать итоговую метрику. Беда будет в том, что она окажется смещённой."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e24fabe",
      "metadata": {
        "id": "4e24fabe"
      },
      "source": [
        "__д) [10 баллов]__ Сделайте $10^4$ выборок размера $1000$. Постройте для каждой оценку доли плохих показов. Считайте, что модератор во время разметки безошибочно определяет значение из колонки `is_spam`.\n",
        "\n",
        "Нарисуйте гистограмму для получившегося распределения. Отметьте на ней настоящую долю плохих показов и получившееся у вас среднее. Правда ли, что они сильно отличаются друг от друга? Найдите среднее смещение оценки."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff992280",
      "metadata": {
        "id": "ff992280"
      },
      "outputs": [],
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
        "# will the code be with you"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b820f32",
      "metadata": {
        "id": "9b820f32"
      },
      "source": [
        "**Откуда появляется это смещение?** Представим себе, что у нас есть случайная величина $X$, которая принмает пять значений с вероятностями\n",
        "\n",
        "<center>\n",
        "\n",
        "|  $X$         | $x_1$   | $x_2$ | $x_3$ | $x_4$    | $x_5$    |\n",
        "|:------------:|:-------:|:-----:|:-----:|:--------:|:--------:|\n",
        "| $P(X = x)$   | $^1/_2$ |$^1/_4$|$^1/_8$|$^1/_{16}$|$^1/_{16}$|\n",
        "\n",
        "</center>\n",
        "\n",
        "Если мы делаем выборку с повторениями, как часто туда будет попадать элемент $x_1$? Элемент попадает в выборку с вероятностью $^1/_2.$ Будем вытаскивать из выборки элементы до тех пор, пока $x_1$ не окажется в наших руках. Номер попытки, начиная с которой $x_1$ окажется у нас, имеет геометрическое распределение. Если $Y \\sim \\text{Geom}(p),$ тогда $\\mathbb{E}(Y) = \\frac{1}{p}.$\n",
        "\n",
        "Получается, элемент $x_1$ окажется в нашей выборке в среднем на второй попытке. Если бы мы делали выборку с повторениями, каждый второй элемент в ней был бы равен $x_1,$  каждый четвёртый был бы равен $x_2$, каждый восьмой был бы равен $x_3$ и так далее. Обратим внимание, что если мы делаем выборку из трёх элементов без повторений, то чаще всего мы будем работать с выборкой $x_1, x_2, x_3$.\n",
        "\n",
        "В ситуации с видео, мы сэмплируем их пропорционально числу показов. В числе показов может быть очень сильный перекос. Какие-то видео показываются в рекомендательной системе десятки раз, а какие-то вирусятся и прорываются в тренды. Каждый раз, когда мы __без возвращения__ берём новое видео в выборку, мы как бы занижаем его вес в несмещённой выборке (выборке с повторениями)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4290bb25",
      "metadata": {
        "id": "4290bb25"
      },
      "source": [
        "#### 5. Исправляем смещение\n",
        "\n",
        "На выборку без повторений размера $n$ можно смотреть следующим образом: мы генерируем выборку с возвращением до тех пор, пока количество уникальных элементов не достигнет числа $n$. При этом, если мы берём на каком-то шаге элемент уже выбранный ранее, мы не включаем его в выборку, а только отдельно запоминаем где-нибудь счётчик числа вхождений этого элемента $c_i$. При таком подходе несмещённую оценку доли плохого можно записать как\n",
        "\n",
        "$$\n",
        "\\hat{p}_{\\text{bad}} = \\frac{\\sum_{i=1}^{n} c_i \\cdot \\text{isSpam}(v_i)}{\\sum_{i=1}^{n} c_i.}\n",
        "$$\n",
        "\n",
        "Если выборка имеет сильно неравномерные веса, то нам придётся довольно долго генерировать элементы с возвращением, пока мы наберём необходимое количество уникальных.\n",
        "\n",
        "Поэтому вместо того, чтобы накапливать счётчики вхождений, мы рассчитаем их математическое ожидание по всем сгенерированным выборкам без повторений, имеющим такое же упорядоченное множество элементов $w_i = \\mathbb{E}(c_i).$\n",
        "\n",
        "Пусть $q_i = \\frac{show(v_i)}{\\sum_{j=1}^{|V|} show(v_j)}$,пусть $w_i^k$ —  количество вхождений элемента с индексом $i$ к моменту, когда в выборке набралось $k$ уникальных элементов. Пусть уникальные элементы попадают в выборку в порядке $v_1, v_2, v_3, \\ldots, v_n$.\n",
        "\n",
        "Когда мы возьмём первый элемент:\n",
        "\n",
        "$$\n",
        "w_1^1 =1, \\quad w_2^1 = 0, \\quad w_3^1 = 0, \\quad \\ldots, \\quad w_n^1 = 0.\n",
        "$$\n",
        "\n",
        "Прежде, чем мы достанем второй уникальный элемент, мы, в среднем, достанем первый элемент ещё  $\\frac{1}{1 - q_1} - 1 = \\frac{q_1}{1 - q_1}$ раз.\n",
        "\n",
        "Эту величину мы посчитали с помощью геометрического распределения. В качестве успешного события мы рассматриваем второй уникальный элемент. Вероятность успеха равна $1- q_1.$ Геометрическая случайная величина представляет из себя номер первого успешного события. Если мы хотим получить число не успешных событий, надо вычесть единицу.\n",
        "\n",
        "Получаем\n",
        "\n",
        "$$\n",
        "w_1^2 = 1 + \\frac{q_1}{1 - q_1}, \\quad w_2^2 = 1, \\quad w_3^2 = 0, \\quad \\ldots, \\quad w_n^2 = 0.\n",
        "$$\n",
        "\n",
        "Прежде, чем мы достанем третий уникальный элемент, мы, в среднем, достанем первые два элемента ещё $\\frac{1}{1 - (q_1 + q_2)} - 1 = \\frac{q_1 + q_2}{1 - (q_1 + q_2)}$ раз. Из них доля доставания первого элемента составляет $\\frac{q_1}{q_1 + q_2}$, а доля второго $\\frac{q_2}{q_1 + q_2}$.\n",
        "\n",
        "Получаем\n",
        "\n",
        "$$\n",
        "w_1^3 = 1 + \\frac{q_1}{1 - q_1} + \\frac{q_1}{1 - (q_1 + q_2)}, \\quad w_2^3 = 1 + \\frac{q_2}{1 - (q_1 + q_2)}, \\quad  w_3^3 = 1, \\quad \\ldots, \\quad w_n^3 = 0.\n",
        "$$\n",
        "\n",
        "Продолжая эту логику до шага $n,$ получаем формулу\n",
        "\n",
        "$$\n",
        "c_i = w_i^n = 1 + q_i \\cdot \\left( \\sum_{j=i}^{n-1} \\frac{1}{1 - \\sum_{k=1}^j q_k} \\right).\n",
        "$$\n",
        "\n",
        "Подставим эти веса вместо счетчиков $c_i$. Это даст нам несмещённую оценку доли «плохих» показов на основе разметки ровно $n$ объектов\n",
        "\n",
        "$$\n",
        "\\hat p_{\\text{bad}} = \\frac{\\sum_{i=1}^n \\left[ 1 + q_i \\cdot \\left( \\sum_{j=i}^{n-1} \\frac{1}{1 - \\sum_{k=1}^j q_k} \\right) \\right] \\cdot isSpam(v_i)}{\\sum_{i=1}^n \\left[ 1 + q_i \\cdot \\left( \\sum_{j=i}^{n-1} \\frac{1}{1 - \\sum_{k=1}^j q_k} \\right)  \\right]},  \\quad q_i = \\frac{show(v_i)}{\\sum_{j=1}^{|V|} show(v_j)}.\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b13032a",
      "metadata": {
        "id": "0b13032a"
      },
      "source": [
        "__е) [10 баллов]__ Пришло время закодит, полученные выше веса. Сделайте $1000$ выборок без повторений размера $1000$. Постройте для каждой оценку доли плохих показов. Убедитесь, что оценка, предложенная выше, окажется несмещённой.\n",
        "\n",
        "**Hint:** для расчёта весов удобно воспользоваться несколько раз функцией `np.cumsum`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3c28596",
      "metadata": {
        "id": "d3c28596"
      },
      "outputs": [],
      "source": [
        "# внимательно изучите этот код:\n",
        "\n",
        "a = np.array([2, 3, 4, 5])\n",
        "np.cumsum(a[:-1][::-1])[::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c389472",
      "metadata": {
        "id": "3c389472"
      },
      "outputs": [],
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
        "# will the code be with you"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90e4c847",
      "metadata": {
        "id": "90e4c847"
      },
      "source": [
        "#### 6. Доверительный интервал\n",
        "\n",
        "Про долю плохих заказов надо уметь делать выводы. Для этого надо построить доверительный интервал. Для него нужна дисперсия. Все карточки размечаются модераторами незвисимо друг от друга. Получается, что дисперсию можно найти как\n",
        "\n",
        "$$\n",
        "\\text{Var}(\\hat p_{\\text{bad}}) = \\frac{\\sum_{i=1}^n c^2_i \\cdot \\text{Var}(\\text{isSpam}(v_i))}{\\left(\\sum_{i=1}^n c_i\\right)^2} = \\frac{\\sum_{i=1}^n c^2_i}{\\left(\\sum_{i=1}^n c_i\\right)^2} \\cdot p_{\\text{bad}} \\cdot (1 - p_{\\text{bad}})\n",
        "$$\n",
        "\n",
        "Видно, что если сэмпл занимает небольшую долю выборки и показы распределены между элементами достаточно равномерно, то веса $c_i$ будут несильно отличаться от $1$. Тогда асимптотически, по мере роста размера сэмпла, стандартное отклонение метрики будет падать как $\\frac{1}{\\sqrt{n}}.$\n",
        "\n",
        "Однако, если существенная доля показов в рекомендательной ленте представлена небольшой группой видосов, тогда веса будут существенно различаться и, начиная с определённого момента, увеличение размера выборки не будет давать заметного снижения разброса в оценке доли плохих показов."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d025111a",
      "metadata": {
        "id": "d025111a"
      },
      "source": [
        "__ё) [5 баллов]__ Сделайте $1000$ выборок без повторений размера $1000$. Постройте по каждой $95\\%$ доверительный интервал для доли плохих показов.\n",
        "\n",
        "Убедитесь, что он действительно покрывает долю плохих показов с вероятностью $0.95$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaeef1bc",
      "metadata": {
        "id": "eaeef1bc"
      },
      "outputs": [],
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
        "# will the code be with you"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6a15e7f",
      "metadata": {
        "id": "f6a15e7f"
      },
      "source": [
        "__ж) [5 баллов]__ Если вы всё сделали всё верно, выше симуляции дали очень плохой результат. Доверительный интервал развалился. Он оказался слишком широким. Более того, он пробивает слева ноль.\n",
        "\n",
        "Сконструируйте доверительный интервал Уилсона. Убедитесь, что он больше не пробивает отрезок $[0;1]$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7633eb0f",
      "metadata": {
        "id": "7633eb0f"
      },
      "outputs": [],
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ▬▬ι═══════  bzzzzzzzzzz\n",
        "# will the code be with you"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94bc15cc",
      "metadata": {
        "id": "94bc15cc"
      },
      "source": [
        "Доверительный интервал Уилсона всё еще не будет давать нужного уровня значимости из-за огромной ширины. Нам нужен какой-то способ уменьшить дисперсию. Это можно сделать с помощью машинного обучения."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34073630",
      "metadata": {
        "id": "34073630"
      },
      "source": [
        "#### 7. Уменьшение дисперсии с помощью машинного обучения\n",
        "\n",
        "Этот раздел вам нужно просто проичитать. Вы уже достаточно настрадались, решая пункты выше.\n",
        "\n",
        "Когда для маленькая, дисперсия оценки доли будет довольно высокой. Относительная ошибка в оценке быстро растёт при уменьшении доли размечаемых видосов, содержащих спам\n",
        "\n",
        "$$\n",
        "\\frac{\\sigma}{p_{\\text{bad}}} \\approx \\frac{\\sqrt{ p_{\\text{bad}} \\cdot (1 -  p_{\\text{bad}})/n} }{ p_{\\text{bad}} } \\approx \\frac{1}{\\sqrt{n \\cdot p_{\\text{bad}}}}.\n",
        "$$\n",
        "\n",
        "Например, если реальная доля показов, содержащих спам составляет $0.5\\%,$ то при разметке выборки из $1000$ видео стандартное отклонение составить $0.22\\%$ и положительную метку будут обычно получать $3-7$ видео, а наша оценка доли спама будет колебаться в пределах $(0.5 \\pm 0.22)\\%.$ При таком уровне шума отслеживать эффект от введения различных улучшений в пайплайнах модерации становится практически невозможно.\n",
        "\n",
        "Однако, если бы у нас существовал способ повысить долю просэмплированных видео с положительной разметкой в сто раз, то мы бы получили оценку доли плохого в сэмпле равную $(50 \\pm 1.6)\\%.$\n",
        "\n",
        "Принимая во внимание, что при построении сэмпла мы завысили долю плохого в сто раз, получаем, что реальная доля плохого составит  $(0.5 \\pm 0.016)\\%.$ То есть с помощью приоритизации мы могли бы снизить разброс примерно в $14$ раз.\n",
        "\n",
        "На практике мы не можем заранее угадать какие видео будут размечены как плохие. Однако у нас есть ML-модели, предсказываютщие подозрительность видео. Например, вероятность того, что видос относится к плохому классу, $\\text{score}(v_i)$.\n",
        "\n",
        "Для того, чтобы сделать размечаемую выборку более представительной по классам, но при этом сохранить репрезентативность по потоку, повысим вероятность сэмплирования видео с высоким уровнем подозрительности.\n",
        "\n",
        "Чтобы после этого по разметке сэмпла оценить долю плохого на потоке, нам нужно обратно изменить веса видео, чтобы величины соотвествовали ожидаемым значениям без перевзвешивания. Новые веса в формуле будут равны\n",
        "\n",
        "$$\n",
        "c_i = \\sum_{i=1}^n  \\frac{1 + q_i \\cdot \\left( \\sum_{j=i}^{n-1} \\frac{1}{1 - \\sum_{k=1}^j q_k}\\right)}{\\text{score}(v_i)}, \\quad q_i = \\frac{\\text{show}(v_i)}{\\sum_{j=1}^{|V|} \\text{show}(v_j)}\n",
        "$$\n",
        "\n",
        "Оценка доли будет искаться как\n",
        "\n",
        "$$\n",
        "\\hat p_{spam} = \\frac{\\sum_{i=1}^n c_i \\cdot \\text{isSpam}(v_i)}{\\sum_{i=1}^n c_i}.\n",
        "$$\n",
        "\n",
        "Аналогично дисперсия такой оценки будет иметь вид\n",
        "\n",
        "$$\n",
        "\\text{Var}(\\hat p_{\\text{bad}}) = \\frac{\\sum_{i=1}^n c^2_i \\cdot \\text{Var}(\\text{isSpam}(v_i))}{\\left(\\sum_{i=1}^n c_i\\right)^2}.\n",
        "$$\n",
        "\n",
        "В формуле дисперсии $\\text{Var}(\\text{isSpam}(v_i))$ уже нельзя считать одинаковыми, так как есть зависимость между весом объекта, обусловленным $score(v_i)$ и дисперсией бернулиевской случайной величины $\\text{isSpam}(v_i).$\n",
        "\n",
        "Если предсказывающая модель обучена в точности на реальном распределении, наблюдаемом в потоке, то можно считать, что $\\text{score}(v_i) = \\mathbb{P}(\\text{isSpam}(v_i) = 1).$\n",
        "\n",
        "Тогда\n",
        "\n",
        "$$\n",
        "\\text{Var}(\\text{isSpam}(v_i)) = \\mathbb{P}(\\text{isSpam}(v_i) = 1) \\cdot \\mathbb{P}(\\text{isSpam}(v_i) = 0) = \\text{score}(v_i) \\cdot (1 - \\text{score}(v_i)).\n",
        "$$\n",
        "\n",
        "Модель должна быть в таком случае [откалибрована.](https://github.com/esokolov/ml-course-hse/blob/master/2022-fall/seminars/sem06-calibration.ipynb)\n",
        "\n",
        "Если откалибровать модель не представляется возможным,тогда можно оценить математическое ожидание и дисперсию элемента на основе исторических данных по разметке элементов с похожими предсказаниями модели.\n",
        "\n",
        "Если для какого-то объекта по каким-то причинам отсутствует $\\text{score}(v_i),$ то при сэмплировании мы можем вставить ему произвольный вес и использовать верхнюю оценку на дисперсию бернуллиевской случайной величины, $\\text{Var}(\\text{isSpam}(v_i)) \\le 0.25.$"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}